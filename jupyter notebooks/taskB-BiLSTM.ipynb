{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"taskB-BiLSTM.ipynb","provenance":[],"authorship_tag":"ABX9TyMmZyT8wLxpfJDVCluMd/kF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"L1h21UKyZmxB"},"outputs":[],"source":["import pandas as pd\n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","url = '/dataset/train.csv'\n","df_train = pd.read_csv(url)\n","df_train = df_train[df_train['final abusive']==1]\n","\n","url = '/dataset/val.csv'\n","df_val = pd.read_csv(url)\n","df_val = df_val[df_val['final abusive']==1]\n","\n","url = '/dataset/test.csv'\n","df_test = pd.read_csv(url)\n","df_test = df_test[df_test['final abusive']==1]\n","\n","df_train = pd.concat([df_train, df_val], ignore_index=True)"]},{"cell_type":"code","source":["def cnv(df):\n","  t=[]\n","  for index in df.index:\n","    if '_' in df['target'][index]:\n","      x = df['target'][index].split('_')\n","      t.append(x)\n","    else:\n","      t.append([df['target'][index]])\n","  df['target2']=t\n","  return df\n","\n","df_train= cnv(df_train)\n","df_test= cnv(df_test)\n","\n","train_x = df_train['sentence'].to_list()\n","test_x = df_test['sentence'].to_list()\n","\n","mlb = MultiLabelBinarizer()\n","train_y = mlb.fit_transform(df_train['target2'])\n","test_y = mlb.transform(df_test['target2'])\n","target_names = list(mlb.classes_)"],"metadata":{"id":"2EHybXrAanOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#importing libraries + pip installing fasttext\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Input, Flatten, MaxPooling1D, SpatialDropout1D, Activation\n","\n","from keras.callbacks import EarlyStopping\n","\n","from numpy import array\n","from sklearn.metrics import classification_report\n","\n","import gensim\n","from gensim import models\n","from gensim.models import Word2Vec\n","\n","!pip install fasttext\n","import fasttext.util"],"metadata":{"id":"Av5lJLMUbvSu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer=Tokenizer(oov_token = \"<OOV>\", split=' ')\n","tokenizer.fit_on_texts(train_x)\n","train_encoded=tokenizer.texts_to_sequences(train_x)\n","train_padded= pad_sequences(train_encoded, padding='post')\n","\n","# padding df_validation \n","test_encoded=tokenizer.texts_to_sequences(test_x)\n","test_padded= pad_sequences(test_encoded, padding='post', maxlen=train_padded.shape[1])"],"metadata":{"id":"pTefYiG0bLBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = train_padded.shape[1]\n","vocabulary_size = len(tokenizer.word_index) + 1\n","EMBEDDING_DIM = 300\n","\n","# function that takes word vector as input and returned an embedding layer\n","def embedding_creation(EMBEDDING_DIM, word_vectors):\n","\n","  vocabulary_size=len(tokenizer.word_index)+1\n","  word_index=tokenizer.word_index\n","  embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n","\n","  for word, i in word_index.items():\n","      try:\n","          embedding_vector=word_vectors[word]\n","          embedding_matrix[i]=embedding_vector\n","      except KeyError:\n","          embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n","\n","  embedding_layer=Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)\n","\n","  return embedding_layer\n","\n","# creating informal_FastText embedding layer (IFT)\n","url = ''\n","wv = fasttext.load_model(url)\n","IFT = embedding_creation(EMBEDDING_DIM, wv)\n","\n","# creating multilingual_FastText embedding layer (MFT)\n","url = ''\n","wv = fasttext.load_model(url)\n","MFT = embedding_creation(EMBEDDING_DIM, wv)\n","del wv\n","\n","# creating BengFastText embedding layer (BFT)\n","url = ''\n","wv = Word2Vec.load(url)\n","BFT = embedding_creation(EMBEDDING_DIM, wv)\n","del wv\n","\n","# creating a randomly initialized embedding layer (RE)\n","RE = Embedding(vocabulary_size, EMBEDDING_DIM,input_length = max_length, trainable=True)"],"metadata":{"id":"WVMx8RwUbK-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_name = []\n","female_p = []\n","female_r = []\n","female_f = []\n","group_p = []\n","group_r = []\n","group_f = []\n","ind_p = []\n","ind_r = []\n","ind_f = []\n","male_p = []\n","male_r = []\n","male_f = []\n","w_p = []\n","w_r = []\n","w_f = []\n","\n","# defining early stopping; stops training when there is no improvement in val_loss for 3 consecutive ecpoch.\n","# returns best model with least val_loss\n","earlystop_callback = EarlyStopping(\n","    monitor=\"val_loss\",\n","    min_delta=0,\n","    patience=3,\n","    verbose=1,\n","    mode=\"min\",\n","    restore_best_weights=True,\n",")"],"metadata":{"id":"EOLsI_p1bK7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_X_name_collection = [ [MFT,'MFT'],[IFT, 'IFT'], [BFT, 'FT'], [RE,'RE'] ]\n","\n","for emb_X_name in emb_X_name_collection:\n","  model = Sequential([\n","      emb_X_name[0],\n","      Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","      GlobalMaxPool1D(),\n","      Dense(16, activation='relu'),\n","      Dense(4, activation='sigmoid'),\n","  ],\n","  name=\"Sentiment_Model\")\n","  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  model.fit(train_padded, train_y, epochs=100, batch_size=32, validation_data=(test_padded,test_y), callbacks=[earlystop_callback])\n","  prediction = model.predict(test_padded)\n","\n","  p=[]\n","  for i in range(len(prediction)):\n","    a=[]\n","    for j in range(4):\n","      a.append(round(prediction[i][j]))\n","    p.append(a)\n","\n","  from sklearn.metrics import classification_report\n","  target_names = list(mlb.classes_)\n","\n","  r = classification_report(test_y, p, target_names=target_names, output_dict=True)\n","\n","  emb_name.append(emb_X_name[1])\n","  female_p.append(round(r['female']['precision']*100, 2))\n","  female_r.append(round(r['female']['recall']*100, 2))\n","  female_f.append(round(r['female']['f1-score']*100, 2))\n","  group_p.append(round(r['group']['precision']*100, 2))\n","  group_r.append(round(r['group']['recall']*100, 2))\n","  group_f.append(round(r['group']['f1-score']*100, 2))\n","  ind_p.append(round(r['ind']['precision']*100, 2))\n","  ind_r.append(round(r['ind']['recall']*100, 2))\n","  ind_f.append(round(r['ind']['f1-score']*100, 2))\n","  male_p.append(round(r['male']['precision']*100, 2))\n","  male_r.append(round(r['male']['recall']*100, 2))\n","  male_f.append(round(r['male']['f1-score']*100, 2))\n","  w_p.append(round(r['weighted avg']['precision']*100, 2))\n","  w_r.append(round(r['weighted avg']['recall']*100, 2))\n","  w_f.append(round(r['weighted avg']['f1-score']*100, 2))\n","\n","  del model"],"metadata":{"id":"fXkvLM-lbK5H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["nh = not hate speech\n","\n","hs = hate speech\n","\n","p = precision\n","\n","r = recall\n","\n","f = f1\n","\n","w: = weighted average"],"metadata":{"id":"jdiVl1NcdXEj"}},{"cell_type":"code","source":["result = {\n","    'model':emb_name,\n","    'female_p':female_p, 'female_r':female_r, 'female_f':female_f,\n","    'group_p':group_p, 'group_r':group_r, 'group_f':group_f,\n","    'ind_p':ind_p, 'ind_r':ind_r, 'ind_f':ind_f,\n","    'male_p':male_p, 'male_r':male_r, 'male_f':male_r,\n","    'w_p':w_p, 'w_r':w_r, 'w_f':w_f\n","}\n","dd = pd.DataFrame(result)\n","dd"],"metadata":{"id":"bJe3r--wbK2X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hw8fxhzCbKz2"},"execution_count":null,"outputs":[]}]}