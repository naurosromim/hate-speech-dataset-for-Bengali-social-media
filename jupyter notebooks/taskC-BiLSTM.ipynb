{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"taskC-BiLSTM.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"15XZM9rRcm6g0ABd55jLoqjz49gaW6uJw","authorship_tag":"ABX9TyNPZ9MDS0ALoNlfFndrUXJd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"H_Jkpe_Xd5KG","executionInfo":{"status":"ok","timestamp":1646381922344,"user_tz":0,"elapsed":13885,"user":{"displayName":"nauros romim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07250597255028260931"}}},"outputs":[],"source":["import pandas as pd\n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","url = '/dataset/train.csv'\n","df_train = pd.read_csv(url)\n","df_train = df_train[df_train['final abusive']==1]\n","\n","url = '/dataset/val.csv'\n","df_val = pd.read_csv(url)\n","df_val = df_val[df_val['final abusive']==1]\n","\n","url = '/dataset/test.csv'\n","df_test = pd.read_csv(url)\n","df_test = df_test[df_test['final abusive']==1]\n","\n","df_train = pd.concat([df_train, df_val], ignore_index=True)"]},{"cell_type":"code","source":["def cnv(df):\n","  t=[]\n","  for index in df.index:\n","    if '_' in df['type'][index]:\n","      x = df['type'][index].split('_')\n","      t.append(x)\n","    else:\n","      t.append([df['type'][index]])\n","  df['type2']=t\n","  return df\n","\n","df_train= cnv(df_train)\n","df_test= cnv(df_test)\n","\n","train_x = df_train['sentence'].to_list()\n","test_x = df_test['sentence'].to_list()\n","\n","mlb = MultiLabelBinarizer()\n","train_y = mlb.fit_transform(df_train['type2'])\n","test_y = mlb.transform(df_test['type2'])\n","target_names = list(mlb.classes_)"],"metadata":{"id":"s3ysH7nmd8lk","executionInfo":{"status":"ok","timestamp":1646381922347,"user_tz":0,"elapsed":13,"user":{"displayName":"nauros romim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07250597255028260931"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#importing libraries + pip installing fasttext\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Input, Flatten, MaxPooling1D, SpatialDropout1D, Activation\n","\n","from keras.callbacks import EarlyStopping\n","\n","import numpy as np\n","from sklearn.metrics import classification_report\n","\n","import gensim\n","from gensim import models\n","from gensim.models import Word2Vec\n","\n","!pip install fasttext\n","import fasttext.util"],"metadata":{"id":"6JFO0nWld8iv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer=Tokenizer(oov_token = \"<OOV>\", split=' ')\n","tokenizer.fit_on_texts(train_x)\n","train_encoded=tokenizer.texts_to_sequences(train_x)\n","train_padded= pad_sequences(train_encoded, padding='post')\n","\n","# padding df_validation \n","test_encoded=tokenizer.texts_to_sequences(test_x)\n","test_padded= pad_sequences(test_encoded, padding='post', maxlen=train_padded.shape[1])"],"metadata":{"id":"FSny8DDDd8gb","executionInfo":{"status":"ok","timestamp":1646381933683,"user_tz":0,"elapsed":33,"user":{"displayName":"nauros romim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07250597255028260931"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["max_length = train_padded.shape[1]\n","vocabulary_size = len(tokenizer.word_index) + 1\n","EMBEDDING_DIM = 300\n","\n","# function that takes word vector as input and returned an embedding layer\n","def embedding_creation(EMBEDDING_DIM, word_vectors):\n","\n","  vocabulary_size=len(tokenizer.word_index)+1\n","  word_index=tokenizer.word_index\n","  embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n","\n","  for word, i in word_index.items():\n","      try:\n","          embedding_vector=word_vectors[word]\n","          embedding_matrix[i]=embedding_vector\n","      except KeyError:\n","          embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n","\n","  embedding_layer=Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)\n","\n","  return embedding_layer\n","\n","# creating informal_FastText embedding layer (IFT)\n","url = ''\n","wv = fasttext.load_model(url)\n","IFT = embedding_creation(EMBEDDING_DIM, wv)\n","\n","# creating multilingual_FastText embedding layer (MFT)\n","url = ''\n","wv = fasttext.load_model(url)\n","MFT = embedding_creation(EMBEDDING_DIM, wv)\n","del wv\n","\n","# creating BengFastText embedding layer (BFT)\n","url = ''\n","wv = Word2Vec.load(url)\n","BFT = embedding_creation(EMBEDDING_DIM, wv)\n","del wv\n","\n","# creating a randomly initialized embedding layer (RE)\n","RE = Embedding(vocabulary_size, EMBEDDING_DIM,input_length = max_length, trainable=True)"],"metadata":{"id":"MJd-QmAOd8dv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_name = []\n","slander_p = []\n","slander_r = []\n","slander_f = []\n","religion_p = []\n","religion_r = []\n","religion_f = []\n","gender_p = []\n","gender_r = []\n","gender_f = []\n","cv_p = []\n","cv_r = []\n","cv_f = []\n","w_p = []\n","w_r = []\n","w_f = []\n","\n","# defining early stopping; stops training when there is no improvement in val_loss for 3 consecutive ecpoch.\n","# returns best model with least val_loss\n","earlystop_callback = EarlyStopping(\n","    monitor=\"val_loss\",\n","    min_delta=0,\n","    patience=3,\n","    verbose=1,\n","    mode=\"min\",\n","    restore_best_weights=True,\n",")"],"metadata":{"id":"XlVoEeppd8bd","executionInfo":{"status":"ok","timestamp":1646382381719,"user_tz":0,"elapsed":517,"user":{"displayName":"nauros romim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07250597255028260931"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["emb_X_name_collection = [ [MFT,'MFT'],[IFT, 'IFT'], [BFT,'BFT'], [RE,'RE'] ]\n","\n","for emb_X_name in emb_X_name_collection:\n","  model = Sequential([\n","      emb_X_name[0],\n","      Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","      GlobalMaxPool1D(),\n","      Dense(16, activation='relu'),\n","      Dense(4, activation='sigmoid'),\n","  ],\n","  name=\"Sentiment_Model\")\n","  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  model.fit(train_padded, train_y, epochs=100, batch_size=32, validation_data=(test_padded,test_y), callbacks=[earlystop_callback])\n","  prediction = model.predict(test_padded)\n","\n","  p=[]\n","  for i in range(len(prediction)):\n","    a=[]\n","    for j in range(4):\n","      a.append(round(prediction[i][j]))\n","    p.append(a)\n","\n","  r = classification_report(test_y, p, target_names=target_names, output_dict=True)\n","  \n","  emb_name.append(emb_X_name[1])\n","  slander_p.append(round(r['slander']['precision']*100, 2))\n","  slander_r.append(round(r['slander']['recall']*100, 2))\n","  slander_f.append(round(r['slander']['f1-score']*100, 2))\n","  religion_p.append(round(r['religion']['precision']*100, 2))\n","  religion_r.append(round(r['religion']['recall']*100, 2))\n","  religion_f.append(round(r['religion']['f1-score']*100, 2))\n","  cv_p.append(round(r['callToViolence']['precision']*100, 2))\n","  cv_r.append(round(r['callToViolence']['recall']*100, 2))\n","  cv_f.append(round(r['callToViolence']['f1-score']*100, 2))\n","  gender_p.append(round(r['gender']['precision']*100, 2))\n","  gender_r.append(round(r['gender']['recall']*100, 2))\n","  gender_f.append(round(r['gender']['f1-score']*100, 2))\n","  w_p.append(round(r['weighted avg']['precision']*100, 2))\n","  w_r.append(round(r['weighted avg']['recall']*100, 2))\n","  w_f.append(round(r['weighted avg']['f1-score']*100, 2))\n","\n","  del model"],"metadata":{"id":"QaShRwzIenXW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["nh = not hate speech\n","\n","hs = hate speech\n","\n","p = precision\n","\n","r = recall\n","\n","f = f1\n","\n","w: = weighted average"],"metadata":{"id":"jdiVl1NcdXEj"}},{"cell_type":"code","source":["result = {\n","    'emb name':emb_name,\n","    'slander_p':slander_p, 'slander_r':slander_r, 'slander_f':slander_f,\n","    'religion_p':religion_p, 'religion_r':religion_r, 'religion_f':religion_f,\n","    'gender_p':gender_p, 'gender_r':gender_r, 'gender_f':gender_f,\n","    'cv_p':cv_p, 'cv_r':cv_r, 'cv_f':cv_f,\n","    'w_p':w_p, 'w_r':w_r, 'w_f':w_f\n","}\n","dd = pd.DataFrame(result)\n","dd"],"metadata":{"id":"bJe3r--wbK2X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6Tq7IkBYenPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hfWyeytjenMf"},"execution_count":null,"outputs":[]}]}