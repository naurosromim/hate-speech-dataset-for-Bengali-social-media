{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"taskB-SVM.ipynb","provenance":[],"authorship_tag":"ABX9TyPe424WsC7AoPp7k+BFL3/N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5dLL-MopZTcl"},"outputs":[],"source":["import pandas as pd\n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","url = '/dataset/train.csv'\n","df_train = pd.read_csv(url)\n","df_train = df_train[df_train['final abusive']==1]\n","\n","url = '/dataset/val.csv'\n","df_val = pd.read_csv(url)\n","df_val = df_val[df_val['final abusive']==1]\n","\n","url = '/dataset/test.csv'\n","df_test = pd.read_csv(url)\n","df_test = df_test[df_test['final abusive']==1]\n","\n","df_train = pd.concat([df_train, df_val], ignore_index=True)"]},{"cell_type":"code","source":["def cnv(df):\n","  t=[]\n","  for index in df.index:\n","    if '_' in df['target'][index]:\n","      x = df['target'][index].split('_')\n","      t.append(x)\n","    else:\n","      t.append([df['target'][index]])\n","  df['target2']=t\n","  return df\n","\n","df_train= cnv(df_train)\n","df_test= cnv(df_test)\n","\n","train_x = df_train['sentence'].to_list()\n","test_x = df_test['sentence'].to_list()\n","\n","mlb = MultiLabelBinarizer()\n","train_y = mlb.fit_transform(df_train['target2'])\n","test_y = mlb.transform(df_test['target2'])\n","target_names = list(mlb.classes_)"],"metadata":{"id":"B2N4OAIZfr6U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","from sklearn.pipeline import make_union\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import LinearSVC\n","import nltk\n","nltk.download('punkt')\n","\n","def token(doc):\n","  import nltk\n","  tokens =nltk.word_tokenize(doc)\n","  return tokens\n","\n","# word unigram TFIDF vectorizer\n","u = TfidfVectorizer(strip_accents=None, tokenizer=token, analyzer='word',  ngram_range=(1,1))\n","\n","# char ngram TFIDF vectorizer\n","c2345 = TfidfVectorizer(strip_accents=None, tokenizer=token, analyzer='char',  ngram_range=(2,5))\n","\n","# word unigram + char ngram TFIDF vectorizer\n","w = TfidfVectorizer(strip_accents=None, tokenizer=token, analyzer='word',  ngram_range=(1,1))\n","c = TfidfVectorizer(strip_accents=None, tokenizer=token, analyzer='char',  ngram_range=(2,5))\n","u_c2345 = make_union(w, c, n_jobs=-1)\n","\n","features = {\n","    'word unigram':u, 'char ngram':c2345, 'word unigram + char ngram':u_c2345\n","}"],"metadata":{"id":"CGvMgPp1frog"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_name = []\n","female_p = []\n","female_r = []\n","female_f = []\n","group_p = []\n","group_r = []\n","group_f = []\n","ind_p = []\n","ind_r = []\n","ind_f = []\n","male_p = []\n","male_r = []\n","male_f = []\n","w_p = []\n","w_r = []\n","w_f = []"],"metadata":{"id":"RRPx4osRfrjL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for feature in features.keys():\n","  vec = features[feature]\n","  train_vector = vec.fit_transform(train_x)\n","  val_vector = vec.transform(test_x)\n","\n","  from sklearn.multiclass import OneVsRestClassifier\n","\n","  clf = OneVsRestClassifier(LinearSVC(penalty='l2', loss='hinge', C=1)).fit(train_vector, train_y)\n","  prediction = clf.predict(val_vector)\n","\n","  r = classification_report(test_y, prediction, target_names=list(mlb.classes_), output_dict=True)\n","  \n","  feature_name.append(feature)\n","  female_p.append(round(r['female']['precision']*100, 2))\n","  female_r.append(round(r['female']['recall']*100, 2))\n","  female_f.append(round(r['female']['f1-score']*100, 2))\n","  group_p.append(round(r['group']['precision']*100, 2))\n","  group_r.append(round(r['group']['recall']*100, 2))\n","  group_f.append(round(r['group']['f1-score']*100, 2))\n","  ind_p.append(round(r['ind']['precision']*100, 2))\n","  ind_r.append(round(r['ind']['recall']*100, 2))\n","  ind_f.append(round(r['ind']['f1-score']*100, 2))\n","  male_p.append(round(r['male']['precision']*100, 2))\n","  male_r.append(round(r['male']['recall']*100, 2))\n","  male_f.append(round(r['male']['f1-score']*100, 2))\n","  w_p.append(round(r['weighted avg']['precision']*100, 2))\n","  w_r.append(round(r['weighted avg']['recall']*100, 2))\n","  w_f.append(round(r['weighted avg']['f1-score']*100, 2))"],"metadata":{"id":"G_G_h_mefrev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = {\n","    'feature name':feature_name,\n","    'female_p':female_p, 'female_r':female_r, 'female_f':female_f,\n","    'group_p':group_p, 'group_r':group_r, 'group_f':group_f,\n","    'ind_p':ind_p, 'ind_r':ind_r, 'ind_f':ind_f,\n","    'male_p':male_p, 'male_r':male_r, 'male_f':male_r,\n","    'w_p':w_p, 'w_r':w_r, 'w_f':w_f\n","}\n","dd = pd.DataFrame(result)\n","dd"],"metadata":{"id":"jFkreFr5gueg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"q66A7pSuguXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7qYrgbnsguPk"},"execution_count":null,"outputs":[]}]}